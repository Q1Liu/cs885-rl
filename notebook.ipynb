{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs885.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQs3rjlNLQ1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXcKkcb3ckIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MDP:\n",
        "  def __init__(self, T, R, discount):\n",
        "    '''\n",
        "    Inputs:\n",
        "    T -- Transition function: |A| x |S| x |S'| array\n",
        "    R -- Reward function: |A| x |S| array\n",
        "    discount -- discount factor: scalar in [0,1)\n",
        "    '''\n",
        "    assert T.ndim == 3\n",
        "    assert (abs(T.sum(2) - 1) < 1e-5).all()\n",
        "    self.nActions = T.shape[0]\n",
        "    self.nStates = T.shape[1]\n",
        "    assert T.shape == (self.nActions, self.nStates, self.nStates)\n",
        "    self.T = T\n",
        "    assert R.ndim == 2\n",
        "    assert R.shape == (self.nActions, self.nStates)\n",
        "    self.R = R\n",
        "    assert 0 <= discount < 1\n",
        "    self.discount = discount\n",
        "    \n",
        "  def valueIteration(self, initialV, nIterations=np.inf, tolerance=0.01):\n",
        "    '''\n",
        "    Bellman optimality equation:\n",
        "    V <-- max_a R^a + gamma T^a V\n",
        "    '''\n",
        "    oldV = initialV\n",
        "    epsilon = np.inf\n",
        "    iteration = 0\n",
        "    while epsilon > tolerance and iteration < nIterations:\n",
        "      newV = np.max(self.R + self.discount * np.einsum('ijk,k->ij', self.T, oldV), axis=0)\n",
        "      epsilon = max(abs(newV - oldV))\n",
        "      oldV = newV\n",
        "      iteration += 1\n",
        "      \n",
        "    return newV\n",
        "  \n",
        "  def extractPolicy(self, V):\n",
        "    '''\n",
        "    Extract greedy policy\n",
        "    pi = argmax(R + gamma * T * V)\n",
        "    '''\n",
        "    policy = np.argmax(self.R + self.discount * np.einsum('ijk,k->ij', self.T, V), axis=0)\n",
        "    \n",
        "    return policy\n",
        "  \n",
        "  def evaluatePolicy(self, policy):\n",
        "    '''\n",
        "    V^pi = R^pi + gamma * T^pi * V^pi\n",
        "    '''\n",
        "    R_pi = np.array([self.R[policy[s]][s] for s in range(self.nStates)])\n",
        "    T_pi = np.array([self.T[policy[s]][s] for s in range(self.nStates)])\n",
        "    \n",
        "    V = np.linalg.solve(np.identity(self.nStates) - self.discount * T_pi, R_pi)\n",
        "    \n",
        "    return V\n",
        "  \n",
        "  def policyIteration(self, initialPolicy, nIterations=np.inf):\n",
        "    '''\n",
        "    Evaluation: solve V^pi = R^pi + gamma * T^pi * V^pi\n",
        "    Improvement: pi = argmax(R + gamma * T * V)\n",
        "    '''\n",
        "    oldPolicy = initialPolicy\n",
        "    iteration = 0\n",
        "    while iteration < nIterations:\n",
        "      V = self.evaluatePolicy(oldPolicy)\n",
        "      newPolicy = self.extractPolicy(V)\n",
        "      iteration += 1\n",
        "      \n",
        "    return newPolicy  \n",
        "     \n",
        "  def evaluatePolicyPartially(self, policy, initialV, nIterations=np.inf, tolerance=0.01):\n",
        "    '''\n",
        "    Repeat V^pi = R^pi + gamma * T^pi * V^pi\n",
        "    instead of solving the system of linear equations with np.linalg.solve()\n",
        "    '''\n",
        "    R_pi = np.array([self.R[policy[s]][s] for s in range(self.nStates)])\n",
        "    T_pi = np.array([self.T[policy[s]][s] for s in range(self.nStates)])\n",
        "    \n",
        "    oldV = initialV\n",
        "    epsilon = np.inf\n",
        "    iteration = 0\n",
        "    while epsilon > tolerance and iteration < nIterations:\n",
        "      newV = R_pi + self.discount * np.matmul(T_pi, oldV)\n",
        "      epsilon = max(abs(newV - oldV))\n",
        "      oldV = newV\n",
        "      iteration += 1\n",
        "      \n",
        "    return newV\n",
        "  \n",
        "  def modifiedPolicyIteration(self, initialPolicy, initialV, nEvalIterations=5, nIterations=np.inf, tolerance=0.01):\n",
        "    '''\n",
        "    The same as policyIteration except that instead of evaluatePolicy we use evaluatePolicyPartially.\n",
        "    '''\n",
        "    policy = initialPolicy\n",
        "    oldV = initialV\n",
        "    epsilon = np.inf\n",
        "    iteration = 0\n",
        "    while epsilon > tolerance and iteration < nIterations:\n",
        "      newV = self.evaluatePolicyPartially(policy, oldV, nEvalIterations, tolerance=tolerance)\n",
        "      policy = self.extractPolicy(newV)\n",
        "      epsilon = max(abs(newV - oldV))\n",
        "      oldV = newV\n",
        "      iteration += 1\n",
        "      \n",
        "    return policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkOp0uI6c1ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "T = np.array([[[0.5,0.5,0,0],[0,1,0,0],[0.5,0.5,0,0],[0,1,0,0]],[[1,0,0,0],[0.5,0,0,0.5],[0.5,0,0.5,0],[0,0,0.5,0.5]]])\n",
        "# Reward function: |A| x |S| array\n",
        "R = np.array([[0,0,10,10],[0,0,10,10]])\n",
        "# Discount factor: scalar in [0,1)\n",
        "discount = 0.9        \n",
        "# MDP object\n",
        "mdp = MDP(T,R,discount)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHcsldmadbWm",
        "colab_type": "code",
        "outputId": "da53bf45-6169-43cd-9135-cea832586fe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "V = mdp.valueIteration(np.zeros((mdp.nStates)), 1000)\n",
        "print(V)\n",
        "policy = mdp.extractPolicy(V)\n",
        "print(policy)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[31.49636306 38.51527513 43.935435   54.1128575 ]\n",
            "[0 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcuYf7s_oCTI",
        "colab_type": "code",
        "outputId": "301d531b-0f62-43a7-b24a-dd74f3b9b6bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "V = mdp.evaluatePolicy(policy)\n",
        "print(V)\n",
        "V = mdp.evaluatePolicyPartially(policy, np.zeros(mdp.nStates), 1000)\n",
        "print(V)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[31.58510431 38.60401638 44.02417625 54.20159875]\n",
            "[31.49636306 38.51527513 43.935435   54.1128575 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0LKYosgolCp",
        "colab_type": "code",
        "outputId": "6aa2bcb6-86a0-4d75-ec82-2d1aff7a9683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "policy = mdp.policyIteration(np.zeros(mdp.nStates, dtype=np.int32), 1000)\n",
        "print(policy)\n",
        "policy = mdp.modifiedPolicyIteration(np.zeros(mdp.nStates, dtype=np.int32), np.zeros(mdp.nStates), 5, 1000)\n",
        "print(policy)\n",
        "\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 1 1]\n",
            "[0 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3URvMFoLxLMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RL:\n",
        "  def __init__(self, mdp, sampleReward):\n",
        "    ''' \n",
        "    Inputs:\n",
        "    mdp -- Markov decision process (T, R, discount)\n",
        "    sampleReward -- Function to sample rewards (e.g., bernoulli, Gaussian).\n",
        "    This function takes one argument: the mean of the distributon and \n",
        "    returns a sample from the distribution.\n",
        "    '''\n",
        "    self.mdp = mdp\n",
        "    self.sampleReward = sampleReward\n",
        "    \n",
        "  def sampleRewardAndNextState(self, state, action):\n",
        "    '''Procedure to sample a reward and the next state\n",
        "    reward ~ Pr(r)\n",
        "    nextState ~ Pr(s'|s,a)\n",
        "    Inputs:\n",
        "    state -- current state\n",
        "    action -- action to be executed\n",
        "    Outputs: \n",
        "    reward -- sampled reward\n",
        "    nextState -- sampled next state\n",
        "    '''\n",
        "    reward = self.sampleReward(self.mdp.R[action, state])\n",
        "    cumProb = np.cumsum(self.mdp.T[action, state, :])\n",
        "    nextState = np.where(cumProb >= np.random.rand(1))[0][0]\n",
        "    return [reward, nextState]\n",
        "  \n",
        "  def selectAction(self, state, Q, epsilon, temperature):\n",
        "    '''\n",
        "    This function implements epsilon exploration if epsilon > 0\n",
        "    if temprature > 0 it would implement Boltzmann exploration.\n",
        "    and if both epsilon and temprature are zero it would implement greedy policy without exploration.\n",
        "    '''\n",
        "    if epsilon > 0:\n",
        "      if np.random.rand(1) < epsilon:\n",
        "        action = np.random.randint(self.mdp.nActions)\n",
        "      else:\n",
        "        action = np.argmax(Q[:, state])\n",
        "        \n",
        "    elif temprature > 0:\n",
        "      probs = [np.exp(Q[action, state] / temprature) for action in range(self.mdp.nActions)]\n",
        "      probs /= np.sum(probs)\n",
        "      action = np.random.choice(self.mdp.nActions, p=probs)\n",
        "      \n",
        "    else:\n",
        "      action = np.argmax(Q[:, state])\n",
        "      \n",
        "    return action\n",
        "\n",
        "  def sampleSoftmaxPolicy(self,policyParams,state):\n",
        "    '''Procedure to sample an action from stochastic policy\n",
        "    pi(a|s) = exp(policyParams(a,s))/[sum_a' exp(policyParams(a',s))])\n",
        "    This function should be called by reinforce() to selection actions\n",
        "\n",
        "    Inputs:\n",
        "    policyParams -- parameters of a softmax policy (|A|x|S| array)\n",
        "    state -- current state\n",
        "\n",
        "    Outputs: \n",
        "    action -- sampled action\n",
        "    '''\n",
        "\n",
        "    denominator = np.exp(policyParams[:,state]).sum()\n",
        "    numerator = np.exp(policyParams[:,state])\n",
        "    pi = numerator / denominator\n",
        "    \n",
        "    action = np.random.choice(self.mdp.nActions, p=pi)\n",
        "    \n",
        "    return action\n",
        "  \n",
        "  def qLearning(self, s0, initialQ, nEpisodes, nSteps, epsilon=0, temperature=0):\n",
        "    '''qLearning algorithm.  Epsilon exploration and Boltzmann exploration\n",
        "    are combined in one procedure by sampling a random action with \n",
        "    probabilty epsilon and performing Boltzmann exploration otherwise.  \n",
        "    When epsilon and temperature are set to 0, there is no exploration.\n",
        "    Inputs:\n",
        "    s0 -- initial state\n",
        "    initialQ -- initial Q function (|A|x|S| array)\n",
        "    nEpisodes -- # of episodes (one episode consists of a trajectory of nSteps that starts in s0\n",
        "    nSteps -- # of steps per episode\n",
        "    epsilon -- probability with which an action is chosen at random\n",
        "    temperature -- parameter that regulates Boltzmann exploration\n",
        "    Outputs: \n",
        "    Q -- final Q function (|A|x|S| array)\n",
        "    policy -- final policy\n",
        "    '''\n",
        "    Q = initialQ\n",
        "    rewards = np.zeros(nEpisodes)\n",
        "    n = np.zeros((self.mdp.nActions, self.mdp.nStates))\n",
        "    for e in range(nEpisodes):\n",
        "      state = s0\n",
        "      for s in range(nSteps):\n",
        "        action = self.selectAction(state, Q, epsilon, temperature)\n",
        "        reward, nextState = self.sampleRewardAndNextState(state, action)\n",
        "        rewards[e] += reward * self.mdp.discount ** s\n",
        "        n[action][state] += 1\n",
        "        alpha = 1.0 / n[action][state]\n",
        "        Q[action, state] += alpha * (reward + self.mdp.discount * np.max(Q[:, nextState]) - Q[action, state])\n",
        "        Q[action][state] = np.round(Q[action][state], 2)\n",
        "        state = nextState\n",
        "        \n",
        "    policy = np.argmax(Q, axis=0)\n",
        "    \n",
        "    return [Q,policy,rewards]\n",
        "\n",
        "\n",
        "  def reinforce(self,s0,initialPolicyParams,nEpisodes,nSteps):\n",
        "    '''reinforce algorithm.  Learn a stochastic policy of the form\n",
        "    pi(a|s) = exp(policyParams(a,s))/[sum_a' exp(policyParams(a',s))]).\n",
        "    This function should call the function sampleSoftmaxPolicy(policyParams,state) to select actions\n",
        "\n",
        "    Inputs:\n",
        "    s0 -- initial state\n",
        "    initialPolicyParams -- parameters of the initial policy (array of |A|x|S| entries)\n",
        "    nEpisodes -- # of episodes (one episode consists of a trajectory of nSteps that starts in s0)\n",
        "    nSteps -- # of steps per episode\n",
        "\n",
        "    Outputs: \n",
        "    policyParams -- parameters of the final policy (array of |A|x|S| entries)\n",
        "    '''\n",
        "\n",
        "    policyParams = initialPolicyParams\n",
        "    n_s_a = np.zeros([self.mdp.nStates, self.mdp.nActions])\n",
        "    rewards = np.zeros(nEpisodes)\n",
        "    for e in range(nEpisodes):\n",
        "      state = s0\n",
        "      G = []\n",
        "      trajectory = []\n",
        "      for s in range(nSteps):\n",
        "        action = self.sampleSoftmaxPolicy(policyParams, state)\n",
        "        n_s_a[state,action] += 1\n",
        "        reward, nextState = self.sampleRewardAndNextState(state, action)\n",
        "        G.append((self.mdp.discount ** s) * reward)\n",
        "        trajectory.append((state, action))\n",
        "        state = nextState\n",
        "      G = np.cumsum(G[::-1])[::-1]\n",
        "      rewards[e] = G[0]\n",
        "      for n in range(nSteps):\n",
        "        state, action = trajectory[n]\n",
        "        alpha = 1 / n_s_a[state,action]\n",
        "        G_n = G[n]\n",
        "        denominator = np.exp(policyParams[:,state]).sum()\n",
        "        for a in range(self.mdp.nActions):\n",
        "          if a == action:\n",
        "            policyParams[a,state] = policyParams[a,state] + 0.1*G_n*(1-np.exp(policyParams[a,state])/denominator)\n",
        "          else:\n",
        "            policyParams[a,state] = policyParams[a,state] + 0.1*G_n*(-np.exp(policyParams[a,state])/denominator)\n",
        "    return [policyParams, rewards]\n",
        "\n",
        "  def modelBasedRL(self,s0,defaultT,initialR,nEpisodes,nSteps,epsilon=0):\n",
        "    '''Model-based Reinforcement Learning with epsilon greedy \n",
        "    exploration.  This function should use value iteration,\n",
        "    policy iteration or modified policy iteration to update the policy at each step\n",
        "\n",
        "    Inputs:\n",
        "    s0 -- initial state\n",
        "    defaultT -- default transition function when a state-action pair has not been vsited\n",
        "    initialR -- initial estimate of the reward function\n",
        "    nEpisodes -- # of episodes (one episode consists of a trajectory of nSteps that starts in s0\n",
        "    nSteps -- # of steps per episode\n",
        "    epsilon -- probability with which an action is chosen at random\n",
        "\n",
        "    Outputs: \n",
        "    V -- final value function\n",
        "    policy -- final policy\n",
        "    '''\n",
        "\n",
        "    T = defaultT\n",
        "    R = initialR\n",
        "    V=np.zeros(self.mdp.nStates)\n",
        "    n_s_a = np.zeros([self.mdp.nStates, self.mdp.nActions])\n",
        "    n_s_a_nextState = np.zeros([self.mdp.nStates, self.mdp.nActions, self.mdp.nStates])\n",
        "    rewards = np.zeros(nEpisodes)\n",
        "    for e in range(nEpisodes):\n",
        "      state = s0\n",
        "      for s in range(nSteps):\n",
        "        action = np.argmax(R[:,state] + self.mdp.discount * np.matmul(T[:,state,:], V))\n",
        "        reward, nextState = self.sampleRewardAndNextState(state, action)\n",
        "        rewards[e] += reward * (self.mdp.discount ** s)\n",
        "        n_s_a[state,action] += 1\n",
        "        n_s_a_nextState[state,action,nextState] += 1\n",
        "        T[action,state,:] = n_s_a_nextState[state,action,:] / n_s_a[state,action]\n",
        "        R[action,state] = (reward + (n_s_a[state,action]-1) * R[action,state]) / n_s_a[state,action]\n",
        "        epsilon = np.inf\n",
        "        while epsilon > 0.01:\n",
        "          V_new = np.max(R + self.mdp.discount * np.einsum('ijk,k->ij', T, V), axis=0)\n",
        "          epsilon = max(abs(V_new-V))\n",
        "          V = V_new\n",
        "        state = nextState\n",
        "    policy = np.argmax(R + self.mdp.discount * np.einsum('ijk,k->ij', T, V), axis=0)\n",
        "\n",
        "    return [V,policy,rewards]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To_05x2qZc-j",
        "colab_type": "code",
        "outputId": "2d039190-692f-4961-ce22-8ccf2d0d805e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "rl = RL(mdp, np.random.normal)\n",
        "\n",
        "# Test Q-learning\n",
        "[Q, policy, rewards] = rl.qLearning(s0=0, initialQ=np.zeros((mdp.nActions, mdp.nStates)), nEpisodes=1000, nSteps=100, epsilon=0.3)\n",
        "print(\"\\nQ-learning results:\")\n",
        "print(Q)\n",
        "print(policy)\n",
        "\n",
        "# Test REINFORCE \n",
        "[policy, rewards] = rl.reinforce(s0=0, initialPolicyParams=np.random.rand(mdp.nActions,mdp.nStates), nEpisodes=1000, nSteps=100)\n",
        "print(\"\\nREINFORCE results:\")\n",
        "print(policy)\n",
        "\n",
        "# Test model-based RL\n",
        "[V, policy, rewards] = rl.modelBasedRL(s0=0, defaultT=np.ones([mdp.nActions,mdp.nStates,mdp.nStates])/mdp.nStates,initialR=np.zeros([mdp.nActions,mdp.nStates]), nEpisodes=1000, nSteps=100, epsilon=0.3)\n",
        "print(\"\\nModel-based RL results:\")\n",
        "print(V)\n",
        "print(policy)\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q-learning results:\n",
            "[[11.72 17.79 25.   28.06]\n",
            " [ 8.74 21.81 27.02 37.36]]\n",
            "[0 1 1 1]\n",
            "\n",
            "REINFORCE results:\n",
            "[[ 4.454234   -3.93237654 -4.32857704 -4.39085301]\n",
            " [-4.34940406  5.52074779  4.99487285  5.42246958]]\n",
            "\n",
            "Model-based RL results:\n",
            "[16.10735194 19.68413259 18.4389601  27.72037902]\n",
            "[0 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67utOUp52fdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Bandit:\n",
        "    def __init__(self,mdp,sampleReward):\n",
        "        '''Constructor for the RL class\n",
        "\n",
        "        Inputs:\n",
        "        mdp -- Markov decision process (T, R, discount)\n",
        "        sampleReward -- Function to sample rewards (e.g., bernoulli, Gaussian).\n",
        "        This function takes one argument: the mean of the distributon and \n",
        "        returns a sample from the distribution.\n",
        "        '''\n",
        "\n",
        "        self.mdp = mdp\n",
        "        self.sampleReward = sampleReward\n",
        "\n",
        "    def sampleRewardAndNextState(self,state,action):\n",
        "        '''Procedure to sample a reward and the next state\n",
        "        reward ~ Pr(r)\n",
        "        nextState ~ Pr(s'|s,a)\n",
        "\n",
        "        Inputs:\n",
        "        state -- current state\n",
        "        action -- action to be executed\n",
        "\n",
        "        Outputs: \n",
        "        reward -- sampled reward\n",
        "        nextState -- sampled next state\n",
        "        '''\n",
        "\n",
        "        reward = self.sampleReward(self.mdp.R[action,state])\n",
        "        cumProb = np.cumsum(self.mdp.T[action,state,:])\n",
        "        nextState = np.where(cumProb >= np.random.rand(1))[0][0]\n",
        "        return [reward,nextState]\n",
        "\n",
        "\n",
        "    def epsilonGreedyBandit(self,nIterations):\n",
        "        '''Epsilon greedy algorithm for bandits (assume no discount factor)\n",
        "\n",
        "        Inputs:\n",
        "        nIterations -- # of arms that are pulled\n",
        "\n",
        "        Outputs: \n",
        "        empiricalMeans -- empirical average of rewards for each arm (array of |A| entries)\n",
        "        '''\n",
        "        \n",
        "        state = 0\n",
        "        n_a = np.zeros(self.mdp.nActions)\n",
        "        empiricalMeans = np.zeros(self.mdp.nActions)\n",
        "        rewards = np.zeros(nIterations)\n",
        "        for i in range(nIterations):\n",
        "                epsilon = 1 / (i + 1)\n",
        "                if np.random.rand(1) < epsilon:\n",
        "                       action = np.random.randint(self.mdp.nActions)\n",
        "                else:\n",
        "                       action = np.argmax(empiricalMeans) \n",
        "                reward, _ = self.sampleRewardAndNextState(state, action)\n",
        "                rewards[i] = reward\n",
        "                empiricalMeans[action] = (n_a[action] * empiricalMeans[action] + reward) / (n_a[action] + 1)\n",
        "                n_a[action] += 1\n",
        "        return empiricalMeans, rewards\n",
        "\n",
        "    def thompsonSamplingBandit(self,prior,nIterations,k=1):\n",
        "        '''Thompson sampling algorithm for Bernoulli bandits (assume no discount factor)\n",
        "\n",
        "        Inputs:\n",
        "        prior -- initial beta distribution over the average reward of each arm (|A|x2 matrix such that prior[a,0] is the alpha hyperparameter for arm a and prior[a,1] is the beta hyperparameter for arm a)  \n",
        "        nIterations -- # of arms that are pulled\n",
        "        k -- # of sampled average rewards\n",
        "\n",
        "        Outputs: \n",
        "        empiricalMeans -- empirical average of rewards for each arm (array of |A| entries)\n",
        "        '''\n",
        "        state = 0\n",
        "        empiricalMeans = np.zeros(self.mdp.nActions)\n",
        "        rewards = np.zeros(nIterations)\n",
        "        for i in range(nIterations):\n",
        "               for a in range(self.mdp.nActions):\n",
        "                      theta = np.random.beta(prior[a,0], prior[a,1], k)\n",
        "                      empiricalMeans[a] = np.mean(theta)\n",
        "               action = np.argmax(empiricalMeans)\n",
        "               reward, _ = self.sampleRewardAndNextState(state, action)\n",
        "               rewards[i] = reward\n",
        "               if reward == 1:\n",
        "                       prior[action,0] += 1\n",
        "               else:\n",
        "                       prior[action,1] += 1\n",
        "\n",
        "        return empiricalMeans, rewards\n",
        "\n",
        "    def UCBbandit(self,nIterations):\n",
        "        '''Upper confidence bound algorithm for bandits (assume no discount factor)\n",
        "\n",
        "        Inputs:\n",
        "        nIterations -- # of arms that are pulled\n",
        "\n",
        "        Outputs: \n",
        "        empiricalMeans -- empirical average of rewards for each arm (array of |A| entries)\n",
        "        '''\n",
        "\n",
        "        state = 0\n",
        "        n = 1\n",
        "        n_a = np.ones(self.mdp.nActions)\n",
        "        empiricalMeans = np.zeros(self.mdp.nActions)\n",
        "        rewards = np.zeros(nIterations)\n",
        "        for i in range(nIterations):\n",
        "                action = np.argmax(empiricalMeans + np.sqrt(2 * np.log(n) / n_a))\n",
        "                reward, _ = self.sampleRewardAndNextState(state, action)\n",
        "                rewards[i] = reward\n",
        "                empiricalMeans[action] = (n_a[action] * empiricalMeans[action] + reward) / (n_a[action] + 1)\n",
        "                n_a[action] += 1\n",
        "                n += 1\n",
        "\n",
        "        return empiricalMeans, rewards\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqBzmAXI55QE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "0631240e-5927-4089-abde-5557bc41f25e"
      },
      "source": [
        "\n",
        "def sampleBernoulli(mean):\n",
        "    ''' function to obtain a sample from a Bernoulli distribution\n",
        "\n",
        "    Input:\n",
        "    mean -- mean of the Bernoulli\n",
        "    \n",
        "    Output:\n",
        "    sample -- sample (0 or 1)\n",
        "    '''\n",
        "\n",
        "    if np.random.rand(1) < mean: return 1\n",
        "    else: return 0\n",
        "\n",
        "\n",
        "# Multi-arm bandit problems (3 arms with probabilities 0.3, 0.5 and 0.7)\n",
        "T = np.array([[[1]],[[1]],[[1]]])\n",
        "R = np.array([[0.3],[0.5],[0.7]])\n",
        "discount = 0.999\n",
        "mdp = MDP(T,R,discount)\n",
        "bandit = Bandit(mdp,sampleBernoulli)\n",
        "\n",
        "# Test epsilon greedy strategy\n",
        "[empiricalMeans, reward] = bandit.epsilonGreedyBandit(nIterations=200)\n",
        "print(\"\\nEpsilonGreedyBandit results:\")\n",
        "print(empiricalMeans)\n",
        "\n",
        "# Test Thompson sampling strategy\n",
        "[empiricalMeans, reward] = bandit.thompsonSamplingBandit(prior=np.ones([mdp.nActions,2]),nIterations=200)\n",
        "print(\"\\nThompsonSamplingBandit results:\")\n",
        "print(empiricalMeans)\n",
        "\n",
        "# Test UCB strategy\n",
        "[empiricalMeans, reward] = bandit.UCBbandit(nIterations=200)\n",
        "print(\"\\nUCBbandit results:\")\n",
        "print(empiricalMeans)\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "EpsilonGreedyBandit results:\n",
            "[0.335 0.    0.   ]\n",
            "\n",
            "ThompsonSamplingBandit results:\n",
            "[0.0055013  0.54223695 0.68615258]\n",
            "\n",
            "UCBbandit results:\n",
            "[0.26086957 0.53846154 0.65217391]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZSV6KRCV8vM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}